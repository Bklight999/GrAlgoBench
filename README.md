

# GrAlgoBench

Large Reasoning Models (LRMs) have achieved rapid progress, yet existing benchmarksâ€”focused primarily on mathematics, programming, or common-sense reasoningâ€”remain limited by **poor difficulty control**, **ambiguous evaluation**, and a **narrow coverage of reasoning paradigms**.  

**GrAlgoBench** introduces a new benchmark centered on **graph algorithm problems** to evaluate the reasoning ability of LRMs. Compared with prior benchmarks, graph tasks offer several unique advantages:  

- **Fine-grained reasoning**: emphasize step-by-step logical execution.  
- **Scalable difficulty control**: adjustable by graph size and topology.  
- **Standardized evaluation**: objective, programmatic correctness checks.  
- **Rich reasoning paradigms**: covering enumeration, exploration, and heuristic decision-making.  

Through experiments on **nine tasks across three categories**, we reveal critical weaknesses of current LRMs:  

1. **Poor intuitive reasoning** â€“ models struggle with heuristic-based tasks.  
2. **Execution errors** â€“ frequent mistakes in step-by-step algorithm execution.  
3. **Limited memory** â€“ difficulty recalling nodes, edges, and intermediate states.  
4. **Over-thinking** â€“ excessive but ineffective self-verification attempts.  

Together, these findings highlight **graph algorithm problems** as a **rigorous, multidimensional, and application-relevant testbed**, exposing the limitations of todayâ€™s LRMs and guiding future progress in reasoning research.  

<p align="center">
  <a href="main_graph.pdf">ðŸ“„ View GrAlgoBench Overview (PDF)</a>
</p>
---

## ðŸ“‚ Project Structure

```
GrAlgoBench/
â”œâ”€â”€ data_generation/        # Scripts for dataset construction
â”œâ”€â”€ Inference/              # Model inference scripts and configs
â”œâ”€â”€ error_analysis/         # Scripts for analyzing model errors
â”œâ”€â”€ logs/                   # Default log directory
â”œâ”€â”€ results/                # Saved inference results
â”œâ”€â”€ scripts/                # Helper bash scripts for batch running
â””â”€â”€ README.md               # Documentation
```

---

## ðŸš€ Quick Start

### 1. Environment Setup
```bash
conda create -n gralgobench python=3.10
conda activate gralgobench
pip install -r requirements.txt
```

### 2. Data Preparation
Place datasets under:
```
/path/to/GrAlgoBench/data_generation/dataset/
```

Datasets are organized as:
```
dataset/
â”œâ”€â”€ MKC_easy.pkl
â”œâ”€â”€ MKC_medium.pkl
â”œâ”€â”€ MST_hard.pkl
â””â”€â”€ ...
```

### 3. Run Inference
Single-task example:
```bash
python Inference/infer_open_large_graph.py \
    --LLM Qwen3-8B \
    --task MST \
    --difficulty medium \
    --batch_size 32 \
    --gpu_num 4
```

Batch execution via script:
```bash
bash scripts/run_all.sh
```

### 4. Results
Outputs are stored under:
```
/path/to/GrAlgoBench/Inference/final_results_{date}/{LLM}/
```

Each run generates a JSON file:
```
MST-medium.json
MKC-easy.json
...
```

### 5. Error Analysis
```bash
python error_analysis/error_analysis.py \
    --task MST \
    --llm o1m \
    --response_generated_from_what_model Qwen3-8B
```

